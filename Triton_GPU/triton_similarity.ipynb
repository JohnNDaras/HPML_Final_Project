{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RB5tl1c-_gql",
        "outputId": "dc9912ed-4893-40df-b340-39c092025849"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting triton\n",
            "  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton) (3.16.1)\n",
            "Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton\n",
            "Successfully installed triton-3.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install triton"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import triton\n",
        "import triton.language as tl\n",
        "from torch import tensor\n",
        "\n",
        "@triton.jit\n",
        "def compute_combinations_kernel(\n",
        "    keys_ptr, areas_ptr, perimeters_ptr, bboxes_ptr, fourier_ptr, num_vertices_ptr, comb_keys_ptr, intersection_areas_ptr, union_areas_ptr,\n",
        "    results1_ptr, results2_ptr, results3_ptr, results4_ptr, results5_1_ptr, results5_2_ptr, results6_ptr, results7_ptr,\n",
        "    final_value_ptr, dictionary1_array_ptr,  # Pointer for expanded dictionary1 array\n",
        "    n_combinations,  # Inputs and outputs\n",
        "    BLOCK_SIZE: tl.constexpr, TENSOR_SIZE: tl.constexpr  # Block size and tensor size\n",
        "):\n",
        "    # Block ID and range of indices this block will handle\n",
        "    pid = tl.program_id(0)\n",
        "    idx = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n",
        "    mask = idx < n_combinations  # Ensure we stay within bounds\n",
        "\n",
        "    # Load offsets for the combination key pairs\n",
        "    offset1 = tl.load(comb_keys_ptr + idx * 2 + 0, mask=mask)\n",
        "    offset2 = tl.load(comb_keys_ptr + idx * 2 + 1, mask=mask)\n",
        "\n",
        "    #--------- CIRCULARITY SIMILARITY ------------#\n",
        "\n",
        "    area_A = tl.load(areas_ptr + offset1, mask=mask)\n",
        "    area_B = tl.load(areas_ptr + offset2, mask=mask)\n",
        "    perimeter_A = tl.load(perimeters_ptr + offset1, mask=mask)\n",
        "    perimeter_B = tl.load(perimeters_ptr + offset2, mask=mask)\n",
        "\n",
        "    result1_1 = (4 * 3.141592653589793 * area_A) / (perimeter_A * perimeter_A)\n",
        "    result1_2 = (4 * 3.141592653589793 * area_B) / (perimeter_B * perimeter_B)\n",
        "    circularity_similarity= 1 / (1 + tl.abs(result1_1 - result1_2))\n",
        "\n",
        "\n",
        "    #--------- PERIMETER SIMILARITY -----------#\n",
        "\n",
        "    perimeter_similarity= 1 / (1 + tl.abs(perimeter_A - perimeter_B))\n",
        "\n",
        "                  # Bounding Boxes\n",
        "    list1_ptr = bboxes_ptr + offset1 * 4  # Each key has 4 items\n",
        "    list2_ptr = bboxes_ptr + offset2 * 4\n",
        "\n",
        "    # Polygon A bounding box\n",
        "    list1_first_item = tl.load(list1_ptr + 0, mask=mask)\n",
        "    list1_second_item = tl.load(list1_ptr + 1, mask=mask)\n",
        "    list1_third_item = tl.load(list1_ptr + 2, mask=mask)\n",
        "    list1_fourth_item = tl.load(list1_ptr + 3, mask=mask)\n",
        "\n",
        "    # Polygon B bounding box\n",
        "    list2_first_item = tl.load(list2_ptr + 0, mask=mask)\n",
        "    list2_second_item = tl.load(list2_ptr + 1, mask=mask)\n",
        "    list2_third_item = tl.load(list2_ptr + 2, mask=mask)\n",
        "    list2_fourth_item = tl.load(list2_ptr + 3, mask=mask)\n",
        "\n",
        "\n",
        "    #--------- ASPECT RATIO SIMILARITY -------------#\n",
        "\n",
        "    aspect_ratio_A = (list1_third_item - list1_first_item) / (list1_fourth_item - list1_second_item)\n",
        "    aspect_ratio_B = (list2_third_item - list2_first_item) / (list2_fourth_item - list2_second_item)\n",
        "    aspect_ratio_sim= 1 / (1 + tl.abs(aspect_ratio_A - aspect_ratio_B))\n",
        "\n",
        "\n",
        "    #---------- BOUNDING BOX DISTANCE --------------#\n",
        "\n",
        "    center_Ax = (list1_first_item - list1_third_item) / 2\n",
        "    center_Ay = (list1_second_item - list1_fourth_item) / 2\n",
        "\n",
        "    center_Bx = (list2_first_item - list2_third_item) / 2\n",
        "    center_By = (list2_second_item - list2_fourth_item) / 2\n",
        "\n",
        "    diff_1 = center_Ax - center_Bx\n",
        "    diff_2 = center_Ay - center_By\n",
        "    bbox_distance_sim= 1 / (1 + tl.sqrt(diff_1 * diff_1 + diff_2 * diff_2))\n",
        "\n",
        "\n",
        "    #----------- FOURIER SIMILARITY ----------------#\n",
        "\n",
        "    tensor1_ptr = fourier_ptr + offset1 * TENSOR_SIZE\n",
        "    tensor2_ptr = fourier_ptr + offset2 * TENSOR_SIZE\n",
        "\n",
        "    sum_of_squares = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n",
        "    for i in range(TENSOR_SIZE):\n",
        "        tensor1_value = tl.load(tensor1_ptr + i, mask=mask)\n",
        "        tensor2_value = tl.load(tensor2_ptr + i, mask=mask)\n",
        "        diff = tensor1_value - tensor2_value\n",
        "        diff_squared = diff * diff\n",
        "        tl.store(dictionary1_array_ptr + idx * TENSOR_SIZE + i, diff_squared, mask=mask)\n",
        "        if i>=1:\n",
        "          val1 = tl.load(dictionary1_array_ptr + idx * TENSOR_SIZE + i, mask=mask)# Store squared difference\n",
        "          val2 = tl.load(dictionary1_array_ptr + idx * TENSOR_SIZE + i-1, mask=mask)\n",
        "          sum_of_squares += val1 + val2\n",
        "\n",
        "    fourier_similarity= 1 / (1 + tl.sqrt(sum_of_squares))\n",
        "\n",
        "\n",
        "    #-------------  JACARD SIMILARITY ----------------#\n",
        "\n",
        "    intersection_area = tl.load(intersection_areas_ptr + idx, mask=mask)\n",
        "    union_area = tl.load(union_areas_ptr + idx, mask=mask)\n",
        "    jacard_similarity= intersection_area / union_area\n",
        "\n",
        "\n",
        "    #-------------- AREA SIMILARITY -------------------#\n",
        "\n",
        "    area_similarity= (2 * union_area) / (area_A + area_B)\n",
        "\n",
        "\n",
        "    #--------------CURVATURE SIMILARITY --------------#\n",
        "\n",
        "    value7_1 = tl.load(num_vertices_ptr + offset1, mask=mask)\n",
        "    value7_2 = tl.load(num_vertices_ptr + offset2, mask=mask)\n",
        "    curvature_similarity= tl.exp(-tl.abs(value7_1 - value7_2) / tl.maximum(value7_1, value7_2))\n",
        "\n",
        "    # -------------------------------Combine Similarity Metrics----------------------------------------#\n",
        "\n",
        "    final_value = 0.125 * (circularity_similarity+ perimeter_similarity+ jacard_similarity+ area_similarity+ aspect_ratio_sim+ bbox_distance_sim+ fourier_similarity+ curvature_similarity)\n",
        "\n",
        "    # Store the results\n",
        "    tl.store(results1_ptr + idx, circularity_similarity, mask=mask)\n",
        "    tl.store(results2_ptr + idx, perimeter_similarity, mask=mask)\n",
        "    tl.store(results3_ptr + idx, jacard_similarity, mask=mask)\n",
        "    tl.store(results4_ptr + idx, area_similarity, mask=mask)\n",
        "    tl.store(results5_1_ptr + idx, aspect_ratio_sim, mask=mask)\n",
        "    tl.store(results5_2_ptr + idx, bbox_distance_sim, mask=mask)\n",
        "    tl.store(results6_ptr + idx, fourier_similarity, mask=mask)\n",
        "    tl.store(results7_ptr + idx, curvature_similarity, mask=mask)\n",
        "    tl.store(final_value_ptr + idx, final_value, mask=mask)  # Store final_value\n",
        "\n",
        "\n",
        "def compute_with_seventh_dir(keys, areas, perimeters, bboxes, fourier, num_vertices, key_lists, intersection_areas_dir, union_areas_dir):\n",
        "    \"\"\"\n",
        "    Perform computations for all combinations of keys in each sublist, including results for directories.\n",
        "    \"\"\"\n",
        "    all_combinations = []\n",
        "    combination_counts = []\n",
        "    total_combinations = 0\n",
        "    intersection_areas_flat = []\n",
        "    union_areas_flat = []\n",
        "\n",
        "    # Generate all combinations for each sublist\n",
        "    for i, sublist in enumerate(key_lists):\n",
        "        combs = torch.combinations(torch.tensor(sublist, device='cuda'), r=2)\n",
        "        all_combinations.append(combs)\n",
        "        combination_counts.append(combs.shape[0])\n",
        "        total_combinations += combs.shape[0]\n",
        "        intersection_areas_flat.extend(intersection_areas_dir[i])\n",
        "        union_areas_flat.extend(union_areas_dir[i])\n",
        "\n",
        "    # Flatten all combinations\n",
        "    all_combinations_flat = torch.cat(all_combinations).flatten()\n",
        "    intersection_areas_flat = torch.tensor(intersection_areas_flat, device='cuda', dtype=torch.float32)\n",
        "    union_areas_flat = torch.tensor(union_areas_flat, device='cuda', dtype=torch.float32)\n",
        "\n",
        "    # Allocate result tensors\n",
        "    results1 = torch.zeros(total_combinations, device='cuda', dtype=torch.float32)\n",
        "    results2 = torch.zeros(total_combinations, device='cuda', dtype=torch.float32)\n",
        "    results3 = torch.zeros(total_combinations, device='cuda', dtype=torch.float32)\n",
        "    results4 = torch.zeros(total_combinations, device='cuda', dtype=torch.float32)\n",
        "    results5_1 = torch.zeros(total_combinations, device='cuda', dtype=torch.float32)\n",
        "    results5_2 = torch.zeros(total_combinations, device='cuda', dtype=torch.float32)\n",
        "    results6 = torch.zeros(total_combinations, device='cuda', dtype=torch.float32)\n",
        "    results7 = torch.zeros(total_combinations, device='cuda', dtype=torch.float32)\n",
        "    final_value = torch.zeros(total_combinations, device='cuda', dtype=torch.float32)\n",
        "\n",
        "    # Allocate memory for expanded dictionary1_array\n",
        "    #dictionary1_array = torch.zeros(total_combinations * TENSOR_SIZE, device='cuda', dtype=torch.float32)\n",
        "\n",
        "    # Launch the kernel\n",
        "    BLOCK_SIZE = 128\n",
        "    TENSOR_SIZE = 10  # Number of elements in each tensor in the sixth directory\n",
        "    dictionary1_array = torch.zeros(total_combinations * TENSOR_SIZE, device='cuda', dtype=torch.float32)\n",
        "    grid = (triton.cdiv(total_combinations, BLOCK_SIZE),)\n",
        "    compute_combinations_kernel[grid](\n",
        "        keys, areas, perimeters, bboxes, fourier, num_vertices, all_combinations_flat, intersection_areas_flat, union_areas_flat,\n",
        "        results1, results2, results3, results4, results5_1, results5_2, results6, results7, final_value, dictionary1_array,\n",
        "        n_combinations=total_combinations,\n",
        "        BLOCK_SIZE=BLOCK_SIZE, TENSOR_SIZE=TENSOR_SIZE\n",
        "    )\n",
        "\n",
        "    # Split results back into sublists\n",
        "    split_results1 = torch.split(results1, combination_counts)\n",
        "    split_results2 = torch.split(results2, combination_counts)\n",
        "    split_results3 = torch.split(results3, combination_counts)\n",
        "    split_results4 = torch.split(results4, combination_counts)\n",
        "    split_results5_1 = torch.split(results5_1, combination_counts)\n",
        "    split_results5_2 = torch.split(results5_2, combination_counts)\n",
        "    split_results6 = torch.split(results6, combination_counts)\n",
        "    split_results7 = torch.split(results7, combination_counts)\n",
        "    split_final_value = torch.split(final_value, combination_counts)\n",
        "\n",
        "    return split_results1, split_results2, split_results3, split_results4, split_results5_1, split_results5_2, split_results6, split_results7, split_final_value\n"
      ],
      "metadata": {
        "id": "gWWi2IJdAORL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import shapely\n",
        "from shapely.geometry import Polygon\n",
        "from shapely.affinity import translate\n",
        "from shapely import intersection, union, get_exterior_ring, get_num_coordinates\n",
        "import torch\n",
        "from itertools import combinations\n",
        "\n",
        "\n",
        "\n",
        "class ShapeSimilarity:\n",
        "    def __init__(self):\n",
        "        self.polygon_dict = {}\n",
        "        self.area_dict = {}\n",
        "        self.perimeter_dict = {}\n",
        "        self.bbox_dict = {}\n",
        "        self.cluster_similarities = []\n",
        "        pass\n",
        "\n",
        "    # Function to center polygons at the origin\n",
        "    def center_polygons(self, polygon_dict):\n",
        "        \"\"\"\n",
        "        Centers polygons in a dictionary by translating each polygon\n",
        "        to have its centroid at the origin.\n",
        "\n",
        "        Parameters:\n",
        "            polygon_dict (dict): A dictionary where values are Shapely polygons.\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary with the same keys but with centered polygons as values.\n",
        "        \"\"\"\n",
        "        centered_polygon_dict = {}\n",
        "        for key, polygon in polygon_dict.items():\n",
        "            centroid = polygon.centroid\n",
        "            centered_polygon = translate(polygon, xoff=-centroid.x, yoff=-centroid.y)\n",
        "            centered_polygon_dict[key] = centered_polygon\n",
        "        return centered_polygon_dict\n",
        "\n",
        "\n",
        "    def sample_boundaries_vectorized(self, polygons, num_points=100):\n",
        "        \"\"\"\n",
        "        Sample evenly spaced points along the exterior boundary of multiple polygons using Shapely's vectorized API.\n",
        "        Args:\n",
        "            polygons: Numpy array of shapely.geometry.Polygon objects.\n",
        "            num_points: Number of points to sample per polygon.\n",
        "        Returns:\n",
        "            A (num_polygons, num_points, 2) numpy array of x, y coordinates.\n",
        "        \"\"\"\n",
        "        # Extract exterior rings for all polygons\n",
        "        exterior_rings = get_exterior_ring(polygons)\n",
        "\n",
        "        # Generate normalized distances for evenly spaced sampling\n",
        "        distances = np.linspace(0, 1, num_points)\n",
        "\n",
        "        # Sample points from the exterior rings\n",
        "        sampled_points = np.array([\n",
        "            np.array([ring.interpolate(d * ring.length).coords[0] for d in distances])\n",
        "            for ring in exterior_rings\n",
        "        ])\n",
        "\n",
        "        return sampled_points\n",
        "\n",
        "    # Define PyTorch Fourier Descriptors Function\n",
        "    def torch_fourier_descriptors(self, contours):\n",
        "        \"\"\"\n",
        "        Compute Fourier Descriptors for a batch of contours using PyTorch.\n",
        "        Args:\n",
        "            contours: Tensor of shape (num_contours, num_points, 2).\n",
        "        Returns:\n",
        "            Tensor of shape (num_contours, num_descriptors).\n",
        "        \"\"\"\n",
        "        complex_contours = torch.complex(contours[:, :, 0], contours[:, :, 1])\n",
        "        fft_result = torch.fft.fft(complex_contours)\n",
        "        return fft_result[:, :100]  # Take the first 10 descriptors\n",
        "\n",
        "\n",
        "    # Step 2: Reconstruct the group_dict from flattened_pairs\n",
        "    def create_group_dict_from_flattened(self, flattened_pairs, cluster_indices, group_sizes):\n",
        "          \"\"\"\n",
        "          Create a dictionary mapping cluster indices to groups of pairs.\n",
        "\n",
        "          Args:\n",
        "              flattened_pairs (list): The 1D list of all pairs across groups.\n",
        "              cluster_indices (list): The indices (keys) for each group.\n",
        "              group_sizes (list): The number of pairs in each group.\n",
        "\n",
        "          Returns:\n",
        "              dict: A dictionary where each cluster index maps to a list of pairs for that group.\n",
        "          \"\"\"\n",
        "          group_dict = {}\n",
        "          start = 0  # Start index for slicing\n",
        "\n",
        "          # Iterate through the group sizes and cluster indices\n",
        "          for cluster_index, size in zip(cluster_indices, group_sizes):\n",
        "              # Extract the pairs for this group\n",
        "              group_dict[cluster_index] = flattened_pairs[start:start + size]\n",
        "              start += size  # Move to the next group\n",
        "\n",
        "          return group_dict\n",
        "\n",
        "\n",
        "\n",
        "    def compute_fourier_descriptors(self, polygons, num_points=100):\n",
        "        \"\"\"\n",
        "        Compute Fourier Descriptors for a set of polygons.\n",
        "\n",
        "        Args:\n",
        "            polygons: List or array of Shapely polygons.\n",
        "            num_points: Number of points to sample along the boundaries.\n",
        "\n",
        "        Returns:\n",
        "            Fourier descriptors as a PyTorch tensor moved to the CPU.\n",
        "        \"\"\"\n",
        "        # Sample evenly spaced points along the polygon boundaries\n",
        "        sampled_contours = self.sample_boundaries_vectorized(polygons, num_points=num_points)\n",
        "\n",
        "        # Convert contours to a PyTorch tensor\n",
        "        contours_tensor = torch.tensor(sampled_contours, dtype=torch.float32)  # Shape: (num_polygons, num_points, 2)\n",
        "\n",
        "        # Move to GPU if available\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        contours_tensor = contours_tensor.to(device)\n",
        "\n",
        "        # Compute Fourier Descriptors\n",
        "        fourier_descriptors = self.torch_fourier_descriptors(contours_tensor)\n",
        "\n",
        "        # Move Fourier Descriptors to CPU\n",
        "        return fourier_descriptors.cpu()\n",
        "\n",
        "\n",
        "\n",
        "    def process_pairs_and_clusters_and_compute_areas(self, list_of_cluster_content_indices, cluster_indices_list):\n",
        "        \"\"\"\n",
        "        Process pairs and clusters, generating unique pairs for each cluster,\n",
        "        and compute intersection and union areas.\n",
        "\n",
        "        Args:\n",
        "            list_of_cluster_content_indices: List of cluster content indices (list of lists).\n",
        "            cluster_indices_list: List of cluster indices corresponding to each cluster.\n",
        "\n",
        "        Returns:\n",
        "            flattened_pairs: Flattened list of unique pairs across all clusters.\n",
        "            group_sizes: Sizes of groups corresponding to the clusters.\n",
        "            intersection_areas: Numpy array of intersection areas for the flattened pairs.\n",
        "            union_areas: Numpy array of union areas for the flattened pairs.\n",
        "        \"\"\"\n",
        "        pairs = []\n",
        "        group_sizes = []\n",
        "\n",
        "        # For each group of keys\n",
        "        for group_keys in list_of_cluster_content_indices:\n",
        "            # Generate all unique pairs for this sublist\n",
        "            sublist_pairs = list(combinations(range(len(group_keys)), 2))\n",
        "\n",
        "            # Separate indices for polygons A and B\n",
        "            idx_A, idx_B = np.array(sublist_pairs).T  # Separate indices for polygons A and B\n",
        "\n",
        "            # Extract the corresponding polygons\n",
        "            polygons_A = np.array([group_keys[i] for i in idx_A])\n",
        "\n",
        "            polygons_B = np.array([group_keys[i] for i in idx_B])\n",
        "\n",
        "            # Combine into pairs for bulk operations\n",
        "            sublist_pairs = list(zip(polygons_A, polygons_B))\n",
        "\n",
        "            # Append this sublist's pairs to the overall pairs structure\n",
        "            pairs.append(sublist_pairs)\n",
        "            group_sizes.append(len(sublist_pairs))\n",
        "\n",
        "        # Create a dictionary where each key corresponds to a cluster index\n",
        "        self.pairs_per_cluster = dict(zip(cluster_indices_list, pairs))\n",
        "        #print(f\"pairs_per_cluster: {self.pairs_per_cluster}\")\n",
        "\n",
        "        # Flatten the pairs into a 1D list\n",
        "        flattened_pairs = [pair for sublist in pairs for pair in sublist]\n",
        "\n",
        "        # Generate all unique pairs\n",
        "        idx_A, idx_B = np.array(flattened_pairs).T  # Separate indices for polygons A and B\n",
        "\n",
        "        # Extract the corresponding polygons\n",
        "        polygons_A = np.array([self.polygon_dict[i] for i in idx_A])\n",
        "        polygons_B = np.array([self.polygon_dict[i] for i in idx_B])\n",
        "\n",
        "        # Combine into pairs for bulk operations\n",
        "        flattened_pairs = list(zip(polygons_A, polygons_B))\n",
        "\n",
        "        # Compute intersections and unions using bulk operations\n",
        "        intersection_areas = shapely.area(intersection(*np.transpose(flattened_pairs)))\n",
        "        union_areas = shapely.area(union(*np.transpose(flattened_pairs)))\n",
        "\n",
        "        return flattened_pairs, group_sizes, intersection_areas, union_areas\n",
        "\n",
        "\n",
        "    def transform_dict_and_sublists(self, input_dict, sublists):\n",
        "        # Step 1: Sort the dictionary by its keys and get a list of (key, value) pairs\n",
        "        sorted_items = sorted(input_dict.items())\n",
        "\n",
        "        # Step 2: Create a new dictionary with keys as indices in the sorted order\n",
        "        new_dict = {index: value for index, (_, value) in enumerate(sorted_items)}\n",
        "\n",
        "        # Step 3: Create a mapping from old keys to new keys\n",
        "        old_to_new_key_mapping = {old_key: index for index, (old_key, _) in enumerate(sorted_items)}\n",
        "\n",
        "        # Step 4: Update the sublists using the mapping\n",
        "        updated_sublists = [\n",
        "            [old_to_new_key_mapping[key] for key in sublist]\n",
        "            for sublist in sublists\n",
        "        ]\n",
        "\n",
        "        return new_dict, updated_sublists\n",
        "\n",
        "\n",
        "    def sort_dictionary(self, input_dict):\n",
        "        # Sort the dictionary by its keys\n",
        "        sorted_items = sorted(input_dict.items())\n",
        "        new_dict = {index: value for index, (_, value) in enumerate(sorted_items)}\n",
        "        return new_dict\n",
        "\n",
        "\n",
        "    def calculate_similarity_for_clusters(self, polygons, polygon_indices, list_of_cluster_content_indices, cluster_indices_list):\n",
        "        \"\"\"\n",
        "        Main method to calculate similarity for multiple clusters.\n",
        "        Args:\n",
        "            polygons: List of shapely polygons.\n",
        "            polygon_indices: List of indices representing the polygons.\n",
        "            list_of_cluster_indices: List of cluster index lists (e.g., [[10, 30], [20, 40]]).\n",
        "        Returns:\n",
        "            List of average similarity scores, one for each cluster.\n",
        "        \"\"\"\n",
        "        # Build polygon dictionary using provided polygon_indices. Each polygon correspond to its index\n",
        "        self.polygon_dict = {index: polygon for index, polygon in zip(polygon_indices, polygons)}\n",
        "\n",
        "        #Ignore dublicates\n",
        "        self.polygon_dict = {key: (value[0] if isinstance(value, (list, tuple)) else value) for key, value in self.polygon_dict.items()}\n",
        "\n",
        "        ########### Precompute all properties using vectorized operations #############\n",
        "\n",
        "        #first center the polygons\n",
        "        self.polygon_dict = self.center_polygons(self.polygon_dict)\n",
        "        all_polygons = list(self.polygon_dict.values())\n",
        "\n",
        "        new_dict, updated_sublists = self.transform_dict_and_sublists(self.polygon_dict, list_of_cluster_content_indices)\n",
        "\n",
        "        #Precompute Areas of all Polygons\n",
        "        print(\"Starting Coputing Areas...\")\n",
        "        all_areas = shapely.area(all_polygons)\n",
        "\n",
        "        #Precompute Lengths of all Polygons\n",
        "        print(\"Starting Computing Lengths\")\n",
        "        all_perimeters = shapely.length(all_polygons)\n",
        "\n",
        "        #Precompute Bounds of all Polygons\n",
        "        print(\"Starting Computing Bounds\")\n",
        "        all_bboxes = shapely.bounds(all_polygons)\n",
        "\n",
        "        # Precompute number of exterior coordinates (vertices)\n",
        "        print(\"Starting computing exterior coords\")\n",
        "        exterior_rings = get_exterior_ring(all_polygons)\n",
        "        self.num_exterior_coords = shapely.get_num_coordinates(exterior_rings)\n",
        "\n",
        "        # Precompute Fourier Descriptors\n",
        "        print(\"Starting computing Fourier Discriptors:\")\n",
        "        self.fourier_descriptors = self.compute_fourier_descriptors(all_polygons)\n",
        "\n",
        "        # Precompute Intersection Areas and Union Areas\n",
        "        flattened_pairs, group_sizes,self.intersection_areas, self.union_areas = self.process_pairs_and_clusters_and_compute_areas(\n",
        "            list_of_cluster_content_indices, cluster_indices_list\n",
        "        )\n",
        "\n",
        "        # Convert precomputed properties to dictionaries\n",
        "        self.area_dict =  dict(zip(polygon_indices, all_areas))\n",
        "        area_dict = self.sort_dictionary(self.area_dict)\n",
        "\n",
        "        self.perimeter_dict = dict(zip(polygon_indices, all_perimeters))\n",
        "        perimeter_dict = self.sort_dictionary(self.perimeter_dict)\n",
        "\n",
        "        self.bbox_dict = dict(zip(polygon_indices, all_bboxes))\n",
        "        bbox_dict = self.sort_dictionary(self.bbox_dict)\n",
        "\n",
        "        self.num_exterior_coords_dict = dict(zip(polygon_indices, self.num_exterior_coords))\n",
        "        num_exterior_coords_dict = self.sort_dictionary(self.num_exterior_coords_dict)\n",
        "\n",
        "        self.fourier_descriptors_dict = dict(zip(polygon_indices, self.fourier_descriptors))\n",
        "        fourier_descriptors_dict = self.sort_dictionary(self.fourier_descriptors_dict)\n",
        "\n",
        "        self.intersection_areas_dict = self.create_group_dict_from_flattened(self.intersection_areas, list(range(len(cluster_indices_list))), group_sizes)\n",
        "        intersection_areas_dict = self.sort_dictionary(self.intersection_areas_dict)\n",
        "\n",
        "        self.union_areas_dict = self.create_group_dict_from_flattened(self.union_areas, list(range(len(cluster_indices_list))), group_sizes)\n",
        "        union_areas_dict = self.sort_dictionary(self.union_areas_dict)\n",
        "\n",
        "\n",
        "        # Convert dictionaries to GPU-compatible tensors\n",
        "        keys = torch.tensor(list(area_dict.keys()), device='cuda', dtype=torch.int32)\n",
        "        values1 = torch.tensor(list(area_dict.values()), device='cuda', dtype=torch.float32)\n",
        "        values2 = torch.tensor(list(perimeter_dict.values()), device='cuda', dtype=torch.float32)\n",
        "        values5 = torch.tensor([v for sublist in bbox_dict.values() for v in sublist], device='cuda', dtype=torch.float32)\n",
        "        values6 = torch.cat([tensor for tensor in fourier_descriptors_dict.values()]).to('cuda').to(torch.float32)\n",
        "        values7 = torch.tensor(list(num_exterior_coords_dict.values()), device='cuda', dtype=torch.float32)\n",
        "\n",
        "        # Run the computation\n",
        "        results1, results2, results3, results4, results5_1, results5_2, results6, results7, final_values= \\\n",
        "            compute_with_seventh_dir(\n",
        "                keys, values1, values2, values5, values6, values7, updated_sublists, intersection_areas_dict, union_areas_dict\n",
        "            )\n",
        "\n",
        "        for i, sublist in enumerate(updated_sublists):\n",
        "\n",
        "            # Compute the average of final values for this sublist\n",
        "            average_final_value = final_values[i].mean().item()\n",
        "            self.cluster_similarities.append(average_final_value)\n",
        "        cluster_similarities_dict = dict(zip(cluster_indices_list, self.cluster_similarities))\n",
        "        # Return the mapping of cluster indices to their respective average similarities\n",
        "        return cluster_similarities_dict\n",
        "\n",
        "\n",
        "# Create example polygons\n",
        "polygons = [\n",
        "    Polygon([(0, 0), (1, 0), (1, 1), (0, 1)]),   # Square\n",
        "    Polygon([(0, 0), (2, 0), (2, 1), (0, 1)]),   # Rectangle\n",
        "    Polygon([(0, 0), (1, 0), (0.5, 1)]),         # Triangle\n",
        "    Polygon([(0, 0), (0.5, 0), (0.25, 0.5)]),    # Small Triangle\n",
        "    Polygon([(0, 0), (1, 0), (0.5, 2)]),          # Tall Triangle\n",
        "    Polygon([(0, 0), (1, 0), (1, 1), (0, 1)])\n",
        "]\n",
        "\n",
        "# Define indices and clusters\n",
        "polygon_indices = [10, 20, 30, 40, 50, 10]                # Custom indices for the polygons\n",
        "list_of_cluster_content_indices = [[10, 30, 40], [20, 50]]    # Two clusters to process\n",
        "cluster_indices_list = [101, 200]  # Arbitrary indices for testing\n",
        "\n",
        "\n",
        "# Instantiate and calculate\n",
        "shape_sim = ShapeSimilarity()\n",
        "similarity_scores = shape_sim.calculate_similarity_for_clusters(polygons, polygon_indices, list_of_cluster_content_indices, cluster_indices_list)\n",
        "for cluster_indices, similarity in similarity_scores.items():\n",
        "    print(f\"Cluster {cluster_indices}: {similarity:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6V69CElBAb5h",
        "outputId": "be14bf29-182c-494b-b92c-7bc07b431cd8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Coputing Areas...\n",
            "Starting Computing Lengths\n",
            "Starting Computing Bounds\n",
            "Starting computing exterior coords\n",
            "Starting computing Fourier Discriptors:\n",
            "Cluster 101: 0.78%\n",
            "Cluster 200: 0.73%\n"
          ]
        }
      ]
    }
  ]
}